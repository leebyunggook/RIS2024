{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyrealsense2 as rs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 컴퓨터에 연결된 카메라의 index 값 확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cameraIndex(n):\n",
    "    arr = []\n",
    "    index = 0\n",
    "    while index < n:\n",
    "        cap = cv2.VideoCapture(index)\n",
    "        if cap.read()[0]:\n",
    "            width = cap.get(cv2.CAP_PROP_FRAME_WIDTH) # Frame Width\n",
    "            height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT) # Frame Height\n",
    "            fps = cap.get(cv2.CAP_PROP_FPS) \n",
    "            print('camera ' + str(index) + ' width: ' + str(width) + ', height: ' + str(height) + ', fps: ' + str(fps))\n",
    "            arr.append(index)\n",
    "            cap.release()\n",
    "        index += 1\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camera 0 width: 640.0, height: 480.0, fps: 30.0\n",
      "camera 2 width: 640.0, height: 480.0, fps: 30.0\n",
      "camera 3 width: 640.0, height: 480.0, fps: 0.0\n",
      "[0, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "out = cameraIndex(5)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/IntelRealSense/librealsense/blob/master/wrappers/python/examples/opencv_viewer_example.py\n",
    "def setRealSensor(): \n",
    "    # Configure depth and color streams\n",
    "    global pipeline, align, dwidth, dheight, fps, depthIntrinsic\n",
    "    pipeline = rs.pipeline()\n",
    "    config = rs.config()\n",
    "\n",
    "    # Get device product line for setting a supporting resolution\n",
    "    pipeline_wrapper = rs.pipeline_wrapper(pipeline)\n",
    "    pipeline_profile = config.resolve(pipeline_wrapper)\n",
    "    device = pipeline_profile.get_device()\n",
    "    device_product_line = str(device.get_info(rs.camera_info.product_line))\n",
    "\n",
    "    found_rgb = False\n",
    "    for s in device.sensors:\n",
    "        if s.get_info(rs.camera_info.name) == 'RGB Camera':\n",
    "            found_rgb = True\n",
    "            break\n",
    "    if not found_rgb:\n",
    "        print(\"The demo requires Depth camera with Color sensor\")\n",
    "        exit(0)\n",
    "\n",
    "    config.enable_stream(rs.stream.depth, dwidth, dheight, rs.format.z16, fps)\n",
    "\n",
    "    if device_product_line == 'L500':\n",
    "        config.enable_stream(rs.stream.color, 960, 540, rs.format.bgr8, fps)\n",
    "    else:\n",
    "        config.enable_stream(rs.stream.color, dwidth, dheight, rs.format.bgr8, fps)\n",
    "\n",
    "    align = rs.align(rs.stream.depth)\n",
    "    pipeline.start(config)\n",
    "\n",
    "    # Get the intrinsics information for calculation of 3D point\n",
    "    frames = pipeline.wait_for_frames()\n",
    "    alignedFrames = align.process(frames)\n",
    "    depth = alignedFrames.get_depth_frame()\n",
    "    depthIntrinsic = depth.profile.as_video_stream_profile().intrinsics\n",
    "\n",
    "    \n",
    "def stopRealSensor():\n",
    "    global pipeline\n",
    "    pipeline.stop()\n",
    "\n",
    "\n",
    "def myRealSensor():\n",
    "    global pipeline, align, frames, alignedFrames \n",
    "    global colorImage, depthImage, alignedColorImage, alignedDepthImage, alignedDepthColormap\n",
    "\n",
    "    frames = pipeline.wait_for_frames()\n",
    "\n",
    "    colorFrame = frames.get_color_frame()\n",
    "    # Convert images to numpy arrays\n",
    "    colorImage = np.asanyarray(colorFrame.get_data())\n",
    "    depthFrame = frames.get_depth_frame()\n",
    "    depthImage = np.asanyarray(depthFrame.get_data())\n",
    "\n",
    "    alignedFrames = align.process(frames)\n",
    "\n",
    "    alignedColorFrame = alignedFrames.get_color_frame()\n",
    "    alignedColorImage = np.asanyarray(alignedColorFrame.get_data())\n",
    "    alignedDepthFrame = alignedFrames.get_depth_frame()\n",
    "    alignedDepthImage = np.asanyarray(alignedDepthFrame.get_data())\n",
    "\n",
    "    # Apply colormap on depth image (image must be converted to 8-bit per pixel first)\n",
    "    # https://velog.io/@csp213/%ED%99%94%EC%86%8C-%EC%B2%98%EB%A6%ACPixel-Point-Processing\n",
    "    convertAlignedDepthImage = cv2.convertScaleAbs(alignedDepthImage, alpha=0.03)\n",
    "    alignedDepthColormap = cv2.applyColorMap(convertAlignedDepthImage, cv2.COLORMAP_JET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture, pipeline, align = 0, 0, 0\n",
    "colorImage, depthImage, alignedColorImage = 0, 0, 0\n",
    "alignedDepthImage, alignedDepthColormap = 0, 0\n",
    "dwidth, dheight, fps = 1280, 720, 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.namedWindow(\"colorImage\", cv2.WINDOW_NORMAL)        \n",
    "cv2.resizeWindow(\"colorImage\", dwidth, dheight)  \n",
    "cv2.namedWindow(\"depthImage\", cv2.WINDOW_NORMAL)        \n",
    "cv2.resizeWindow(\"depthImage\", dwidth, dheight)  \n",
    "cv2.namedWindow(\"alignedColorImage\", cv2.WINDOW_NORMAL)        \n",
    "cv2.resizeWindow(\"alignedColorImage\", dwidth, dheight)  \n",
    "cv2.namedWindow(\"alignedDepthImage\", cv2.WINDOW_NORMAL)        \n",
    "cv2.resizeWindow(\"alignedDepthImage\", dwidth, dheight)  \n",
    "cv2.namedWindow(\"alignedDepthColormap\", cv2.WINDOW_NORMAL)        \n",
    "cv2.resizeWindow(\"alignedDepthColormap\", dwidth, dheight)  \n",
    "\n",
    "setRealSensor()\n",
    "while(True):\n",
    "    myRealSensor()\n",
    "    cv2.imshow(\"colorImage\", colorImage)\n",
    "    cv2.imshow(\"depthImage\", depthImage)\n",
    "    cv2.imshow(\"alignedColorImage\", alignedColorImage)\n",
    "    cv2.imshow(\"alignedDepthImage\", alignedDepthImage)\n",
    "    cv2.imshow(\"alignedDepthColormap\", alignedDepthColormap)\n",
    "    key = cv2.waitKey(1) & 0xFF # delay_time\n",
    "    if key == ord('q') or key == 27:\n",
    "        break\n",
    "stopRealSensor()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
